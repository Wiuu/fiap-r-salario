---
title: "CONCEITOS ESTATÍSTICOS PARA IA"
output:
  html_document: 
    code_folding: hide
    fig_caption: yes
    fig_height: 7.0
    fig_width: 14.0
    highlight: tango
    number_sections: yes
    theme: cerulean
    toc: yes
---

```{r load libraries, message=FALSE, warning=FALSE}
library(corrplot)
library(corrgram)
library(skimr)
library(knitr)
library(ggplot2)
library(dplyr)
library(formattable)
library(randomForest)
library(caret)
library(readr)
library(gmodels)
library(rpart)
library(rpart.plot)
library(polycor)
library(cluster)
library(fpc)
library(readxl)
```
******
# Introdução
******

Esta analise visa utilizar o dataset disponivel e mensurar a variavel salarios com base em algumas variaveis, como: sexo, idade, tamanho da empresa e outros.

******
# Carregando os dados
******

```{r load data}
salarios <- read_excel(path = './salario_1.xlsx', sheet = 1);
salarios$sexo <- factor(salarios$sexo);
salarios
```

******
# Sumario da base
******
```{r load data}
summary(salarios)
str(salarios, stringsAsFactors = TRUE)
```

Dataset com 6 variaveis, dentre elas a variável "salario" indica o valor de salario por cada funcionário
 - **salario:** Indica o valor recebido de salario
 - **sexo:** Indica o genero masculino (2) feminino (1)
 - **tempoempresa:** Indica o tempo de empresa de cada funcionário;
 - **idade:** Indica a idade de cada funcionário;
 - **escolaridade:** Indica o grau de escolaridade de cada funcionário;
 - **experiencia:** Indica o grau de experiência de cada funcionário;

******
## Verificando a amostra
******

Analisando a consistencia da amostra em relação a dados incompletos ou faltantes:
```{r results='asis'}
skim(salarios) %>% skimr::kable()
```

******
## Detectando Outliers
******

Outliers são observações que apresentam uma grande diferenciação ou inconsistencias em relação aos demais. Para isso usamos analise graficas com sobreposição de histograma x distribuição normal e delimitacão de linha de corte com boxplot.

```{r}
hist.default <- function(x,
                         breaks = "Sturges",
                         freq = NULL,
                         include.lowest = TRUE,
                         normalcurve = TRUE,
                         right = TRUE,
                         density = NULL,
                         angle = 45,
                         col = NULL,
                         border = NULL,
                         main = paste("Histogram of", xname),
                         ylim = NULL,
                         xlab = xname,
                         ylab = NULL,
                         axes = TRUE,
                         plot = TRUE,
                         labels = FALSE,
                         warn.unused = TRUE,
                         ...)  {
  xname <- paste(deparse(substitute(x), 500), collapse = "\n")

  suppressWarnings(
    h <- graphics::hist.default(
      x = x,
      breaks = breaks,
      freq = freq,
      include.lowest = include.lowest,
      right = right,
      density = density,
      angle = angle,
      col = col,
      border = border,
      main = main,
      ylim = ylim,
      xlab = xlab,
      ylab = ylab,
      axes = axes,
      plot = plot,
      labels = labels,
      warn.unused = warn.unused,
      ...
    )
  )

  if (normalcurve == TRUE & plot == TRUE) {
    x <- x[!is.na(x)]
    xfit <- seq(min(x), max(x), length = 40)
    yfit <- dnorm(xfit, mean = mean(x), sd = sd(x))
    if (isTRUE(freq) | (is.null(freq) & is.null(density))) {
      yfit <- yfit * diff(h$mids[1:2]) * length(x)
    }
    lines(xfit, yfit, col = "black", lwd = 2)
  }

  if (plot == TRUE) {
    invisible(h)
  } else {
    h
  }
}

plotaGraficos <- function(fsalario, label){
par(mfrow = c(1,2))
hist(fsalario, main = paste("Histograma de ",label), xlab = label, ylab="Frequência")
abline(v = mean(fsalario) - 2 * sd(fsalario), col = "red")
abline(v = mean(fsalario) + 2 * sd(fsalario), col = "red")
boxplot(fsalario)
}

plotaGraficos(salarios$salario, "salario")
plotaGraficos(salarios$tempoempresa, "tempoempresa")
plotaGraficos(salarios$idade, "idade")
plotaGraficos(salarios$escolaridade, "escolaridade")
plotaGraficos(salarios$experiencia, "experiencia")
```

## Removendo Outliers
Pelo boxplot é possível visualizar que há grupos distintos de salários, mas também é possível notar que neste, existem observações com valores muito distantes dos agrupamentos no gráfico e consideramos estas possíveis outliers, sendo assim, serão removidos para que não interfiram no resultado da análise e dos algoritmos
```{r}
salarios <- salarios%>%filter(salario < 26000)
plotaGraficos(salarios$salario, "salario")
```

Aqui vemos uma diminuição nas entradas do dataset após remoção de outliers
```{r}
summary(salarios)
str(salarios, stringsAsFactors = TRUE)
```

******
# Correlação de variaveis
******

Gerando a correlação das variaveis, vai permitir o entendimento de quais carácteristicas influenciam mais no valor do salário recebido.

Vamos começar pela matriz de correlação.

## Matriz de Correlação

 Matriz de correlação mostra os valores de correlação de Pearson, que medem o grau de relação linear entre cada par de itens ou variáveis. Os valores de correlação podem cair entre -1 e +1.
```{r}
matcor <- hetcor(salarios%>%select(2:7))
panel.cor <- function(x, y, digits=2, prefix ="", cex.cor,
                      ...)  {
  usr <- par("usr")
  on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  r <- cor(x, y , use = "pairwise.complete.obs")
  txt <- format(c(r, 0.123456789), digits = digits) [1]
  txt <- paste(prefix, txt, sep = "")
  if (missing(cex.cor))
    cex <- 0.8/strwidth(txt)
  text(0.5, 0.5, txt, cex = cex * abs(r))
}
pairs(salarios%>%select(2:7), lower.panel=panel.smooth, upper.panel=panel.cor)
```
### Conclusão

O Valor de um salário neste estudo, apresenta correlação com a Escolaridade, tempo empresa, sexo e idade, sendo o maior influenciador a escolaridade, ainda que não seja tão forte assim.

## Estimando o salário
Tendo como alvo o salario, usaremos os dados aqui presentes para levantarmos e estimarmos um salário usando 2 técnicas.

### Regressão Linear
A regressão linear consiste em uma função que relaciona as variáveis que contém características do objeto de estudo a uma varíavel depente do mesmo objeto de estudo, gerando assim uma relação entre o resultado observado às suas possíveis explicações.

Usaremos o método stepwise no desenvolvimento do modelo, selecionando assim as variáveis que melhor estimem o valor do salário.

```{r, message=FALSE, warning=FALSE}
salarios_rl <- salarios%>%select(2:7)
modelo_rl <- lm(salarios_rl$salario ~ salarios_rl$sexo + salarios_rl$tempoempresa + salarios_rl$idade + salarios_rl$escolaridade + salarios_rl$experiencia);
stepwise<-step(modelo_rl,direction="both")
summary(stepwise)

```
Através da sumarização do modelo podemos observar que escolaridade, tempo empresa, experiencia tem maior influencia no salario em comparação a idade e sexo, testes com menor influencia.

******
#### Análise de resíduos

******
Com o gráfico abaixo podemos concluir que os resíduos da predição com o modelo desenvolvido que a premissa de normalidade é atendida:
```{r}
qqnorm(residuals(modelo_rl), ylab="Resíduos",xlab="Quantis teóricos",main="")
qqline(residuals(modelo_rl))
```
Concluímos que com o modelo de regressão desenvolvido temos um erro quadrático médio de aproximadamente 2737.
```{r}
pred <- predict(modelo_rl,interval = "prediction", level = 0.95) 
fit <- pred[,1]
mse <- mean((salarios_rl$salario  - fit)^2)
sqrt(mse)
```

### Árvore de Regressão
Árvores de Regressão são idênticas às árvores de decisão porém para variáveis escalares, na figura abaixo está a plotagem de uma árvore de regressão de 9 níveis na qual as folhas agrupam os funcionarios por salario, o split do algoritmo foi setado em 33 que é aproximadamente o valor de 5% da amostra. Com este algorimo atingimos um erro quadrático médio de aproximadamente 2737.
```{r}
modelo_arvore <- rpart(salario ~ sexo + tempoempresa + idade + escolaridade + experiencia, data=salarios_rl, 
                     cp = 0.001,minsplit = 33,maxdepth=20)

rpart.plot(modelo_arvore, type=4, extra=1, under=FALSE, clip.right.labs=TRUE,
           fallen.leaves=FALSE,   digits=2, varlen=-10, faclen=20,
           cex=0.4, tweak=1.7,
           compress=TRUE, 
           snip=FALSE)

pred_arvore <- predict(modelo_arvore,interval = "prediction", level = 0.95) 
mse_tree <- mean((salarios_rl$salario  - pred_arvore)^2)
sqrt(mse_tree)
```

Assim concluímos que entre os dois algoritmos apresentados o de melhor desempenho foi a <b>Árvore de Regressão</b> ainda que com uma diferença muito pequena entre eles.

## Classificando salários
Tendo em mente salarios maiores ou iguais à 6.000 sãos classificados com ALTOS e os inferiores à isso são classificados como BAIXOS podemos criar uma variável CATEGORIA_SALARIAL e criarmos uma nova variável no dataset e trabalharmos sobre os dados que levam a esta classificação.
```{r}
salario_class <- salarios%>%
                select(2:7)%>%
                mutate(CATEGORIA_SALARIAL = ifelse(salario >= 6000, "ALTO", "BAIXO"))
salario_class$CATEGORIA_SALARIAL <- factor(salario_class$CATEGORIA_SALARIAL)

summary(salario_class$CATEGORIA_SALARIAL)
```

### Entendendo a relação das variáveis
Plotamos alguns gráficos para entendermos a relação das variáveis com a categorização atribuída aos salários:
```{r}
#comando para gerar em 4 linhas e duas colunas os plots
par (mfrow=c(1,2))
plot(salario_class$CATEGORIA_SALARIAL, salario_class$salario,ylab="salario",xlab="Categoria",col=c('red','darkgreen'))
plot(salario_class$CATEGORIA_SALARIAL, salario_class$escolaridade,ylab="escolaridade",xlab="Categoria",col=c('red','darkgreen'))
plot(salario_class$CATEGORIA_SALARIAL, salario_class$tempoempresa,ylab="tempoempresa",xlab="Categoria",col=c('red','darkgreen'))
plot(salario_class$CATEGORIA_SALARIAL, salario_class$experiencia,ylab="experiencia",xlab="Categoria",col=c('red','darkgreen'))
plot(salario_class$CATEGORIA_SALARIAL, salario_class$idade,ylab="idade",xlab="Categoria",col=c('red','darkgreen'))
plot(salario_class$CATEGORIA_SALARIAL, salario_class$sexo,ylab="sexo",xlab="Categoria",col=c('red','darkgreen'))
```

Agora vamos usar o corrplot para enxergarmos as correlações.
```{r}
matcor <- hetcor(salario_class)
panel.cor <- function(x, y, digits=2, prefix ="", cex.cor,
                      ...)  {
  usr <- par("usr")
  on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  r <- cor(x, y , use = "pairwise.complete.obs")
  txt <- format(c(r, 0.123456789), digits = digits) [1]
  txt <- paste(prefix, txt, sep = "")
  if (missing(cex.cor))
    cex <- 0.8/strwidth(txt)
  text(0.5, 0.5, txt, cex = cex * abs(r))
}
pairs(salario_class, lower.panel=panel.smooth, upper.panel=panel.cor)
```
As variáveis que apresentam os maiores graus de correção com a Categoria_salario (ALTO ou BAIXO) ainda que negativas são teor Salário, sexo e escolaridade.

### Categorizando os salarios
A fim de categorizar os salários a partir de suas características utilizaremos as técnicas de regressão logística e árvore de decisão. Para o uso de tais técnicas dividiremos nosso dataset em 2/3 para o treinamento e 1/3 para a validação.

```{r}
particao <- 2/3
set.seed(2019)
treino <- sample(1:NROW(salario_class), as.integer(particao*NROW(salario_class)))

trainData <- salario_class[treino,]
testData  <- salario_class[-treino,]
```


#### Árvore de decisão
Com o intuito de selecionarmos as variáveis que utilizaremos na regressão logística primeiros iremos usar a árvore de decisão para identificarmos quais variáveis se apresentam como critérios de decisão.

```{r}
modelo_arvore_decisao <- rpart (CATEGORIA_SALARIAL ~ sexo + tempoempresa + idade + escolaridade + experiencia, data=trainData, cp = 0.006,minsplit = 33,maxdepth=20)
rpart.plot(modelo_arvore_decisao, type=4, extra=104, under=FALSE, clip.right.labs=TRUE,
           fallen.leaves=FALSE,   digits=2, varlen=-3, faclen=20,
           cex=0.4, tweak=1.7,
           compress=TRUE,
           snip=FALSE)

salario_predito <- predict(modelo_arvore_decisao ,testData , type = "class")

matriz.de.confusao<-table(testData$CATEGORIA_SALARIAL, salario_predito)
matriz.de.confusao

diagonal <- diag(matriz.de.confusao)
perc.erro <- 1 - sum(diagonal)/sum(matriz.de.confusao)
perc.erro
```

Observamos pela matriz de confusão a assertividade do modelo e seu percentual de erro de aproximadamente 18%.

### Regressão Logística
Assim como na regressão linear a regressão logística se baseia em váriáveis independentes para chegar a uma variável dependente, porém neste caso, uma variável categórica
```{r}
modelo_log<-glm(Qualidade ~ fixedacidity + volatileacidity + citricacid  + residualsugar + chlorides + freesulfurdioxide + totalsulfurdioxide + density  + pH + sulphates + alcohol,trainData, family=binomial(link=logit))

predito<-fitted(modelo_log)
hist(predito)
fx_predito <- cut(predito, breaks=c(0,0.10,0.20,0.30,0.40,0.50,0.60,0.70,0.80,0.90,1), right=F)
plot(fx_predito , trainData$Qualidade)

Predito_teste<-predict(modelo_log, testData)

fx_predito1 <- cut(Predito_teste, breaks=c(0,0.50,1), right=F)

MC <- table(testData$Qualidade,  fx_predito1 , deparse.level = 2)
show(MC) 
perc.erro.log = 1 - sum(diag(MC))/sum(MC) 
show(perc.erro.log )
```
Como podemos ver nos gráficos acima e também na matriz de confusão, o percentual de erro do modelo foi de aproximadamente 47%, o que nos leva a concluir que para a classificação dos vinhos entre BOM e RUIM o melhor método foi a <b>Árvore de decisão</b>.

## Definindo grupos de vinhos
Para análise dos grupos de vinhos a partir de sua características físico-químicas iremos utilizar três técnicas, são elas: Clusteres Hierárquicos, Componentes principais e K-means.

```{r}
vinhos_clusters <- vinhos%>%select(2:12)
matcor <- cor(vinhos_clusters)
panel.cor <- function(x, y, digits=2, prefix ="", cex.cor,
                      ...)  {
  usr <- par("usr")
  on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  r <- cor(x, y , use = "pairwise.complete.obs")
  txt <- format(c(r, 0.123456789), digits = digits) [1]
  txt <- paste(prefix, txt, sep = "")
  if (missing(cex.cor))
    cex <- 0.8/strwidth(txt)
  text(0.5, 0.5, txt, cex = cex * abs(r))
}
pairs(vinhos_clusters, lower.panel=panel.smooth, upper.panel=panel.cor)
```
Como já observado anteriormente, algumas das variáveis do dataset apresentam forte correlação entre si como freesulfordioxide e totalsulfordioxide, outro exemplo de forte correlação é alcohol e density. 

### Componentes Principais
Esta técnica consiste no desenvolvimento de componentes de análise que representem as características dos dados sem que que sofre interferência das variáveis que possuem forte correlação entre elas, possibilitando assim um melhor desempenho dos modelos.

```{r}
vinhos_clusters_padr <- scale(vinhos_clusters)
pcacor_vinhos <- prcomp(vinhos_clusters_padr, scale = TRUE) 
summary(pcacor_vinhos)
plot(1:11, pcacor_vinhos$sdev^2, type = "b", xlab = "Componente", ylab = "Variância", pch = 20, cex.axis = 0.8, cex.lab = 0.8)

pcacor_vinhos$rotation[, 1:11]
pcacor_vinhos <- prcomp(vinhos_clusters_padr, scale = TRUE, retx = TRUE)

CP1 <- pcacor_vinhos$x[, 1]
CP2 <- pcacor_vinhos$x[, 2]
CP3 <- pcacor_vinhos$x[, 3]
CP4 <- pcacor_vinhos$x[, 4]
CP5 <- pcacor_vinhos$x[, 5]
CP6 <- pcacor_vinhos$x[, 6]
CP7 <- pcacor_vinhos$x[, 7]
CP8 <- pcacor_vinhos$x[, 8]

par (mfrow=c(1,2))
hist(CP1)
hist(CP2)
hist(CP3)
hist(CP4)
hist(CP5)
hist(CP6)
hist(CP7)
hist(CP8)

pca_vinhos <-cbind(CP1,CP2,CP3,CP4,CP5,CP6,CP7,CP8)
```
Baseado no gráfico de Variãncia x Componente percebos que por volta de 8 componentes principais a variância diminui bastante, e interpretando os componentes percebos que cada um leva em conta com um peso maior seja positivo ou negativo cerca de 2 a 3 características físico-químicas dos vinhos. Adotaremos então os componentes de 1 a 8.

### Clusters Hierárquicos
Assim como o próprio nome diz esta técnica demonstra a hierarquia entre os clusters dentro de um mesmo dataset

```{r}
hier_cluster<-hclust(dist(vinhos_clusters_padr),method='ward.D2')
d <- dist(vinhos_clusters_padr, method = "euclidean") 
plot(hier_cluster, ylab='distancia', cex=0.6)

groups <- cutree(hier_cluster, k=10) 
rect.hclust(hier_cluster, k=10, border="red") 

groups <- cutree(hier_cluster, k=7) 
rect.hclust(hier_cluster, k=7, border="blue")
```
Nos baseando nas características físico-químicas dos vinhos encontramos 10 clusters distintos, sendo alguns dele muito menores do que os demais e outros bem abrangentes, faz parte da avaliação compararmos os clusters encontrados com os componentes principais e procurarmos semelhanças. Reduzimos então o número de clusters para 7 (em azul) para que possamos obersvar a hierarquia.

```{r}
hier_cluster_pca<-hclust(dist(pca_vinhos),method='ward.D2')
d <- dist(pca_vinhos, method = "euclidean") 
plot(hier_cluster_pca, ylab='distancia', cex=0.6)

groups <- cutree(hier_cluster_pca, k=10) 
rect.hclust(hier_cluster_pca, k=10, border="red") 

groups <- cutree(hier_cluster_pca, k=7) 
rect.hclust(hier_cluster_pca, k=7, border="blue")
```
Existe uma diferença entre os clusters encontrados com os componentes principais e podemos inferir que esse diferença se se deve ao motivo dos componentes não sofrem com a alta correlação entre as variáveis. Podemos observar também que a diferença se repete mesmo quando diminuimos a quantidade de clusters, entretanto, quando usamos os componentes principais é perceptível a existência de apenas um grupo muito pequeno o que não ocorre quando nos baseamos nas variáveis explicativas do dataset.

### K-means
É uma técnica não hierárquica que consiste na formação de clusters que agrupem observações a partir de um ponto central baseado na distância da observação ao ponto central, ao fim do k-means as observações estarão clusterizadas em torno do centróido ao qual tem a menor distância.

```{r}
wss <- (nrow(vinhos_clusters_padr)-1)*sum(apply(vinhos_clusters_padr,2,var))
for (i in 2:100) wss[i] <- sum(kmeans(vinhos_clusters_padr,
                                     centers=i, iter.max = 50)$withinss)
plot(1:100, wss, type="b", xlab="Número de clusters") 
```

Analisando o gráfico acima podemos verificar que com cerca 90 clusters teremos poucas diferenças entre observações que indiquem a existe de um novo cluster com características muito específicas.

```{r}
set.seed(2019)
output_cluster<-kmeans(vinhos_clusters_padr,90, iter = 50)
cluster_vinho<-output_cluster$cluster

table (cluster_vinho)

clusplot(vinhos_clusters_padr, output_cluster$cluster, color=TRUE, shade=TRUE,
         labels=2, lines=0 , cex=0.75)

plotcluster(vinhos_clusters_padr, output_cluster$cluster) 
```

```{r}
wss <- (nrow(pca_vinhos)-1)*sum(apply(pca_vinhos,2,var))
for (i in 2:100) wss[i] <- sum(kmeans(pca_vinhos,
                                     centers=i, iter.max = 50)$withinss)
plot(1:100, wss, type="b", xlab="Número de clusters") 
```
Podemos observar que executando o k-means com os componentes principais encontramos cerca 80 clusters com tamanhos diferentes dos encontrados anteriormente como podemos ver abaixo.
```{r}
set.seed(2019)
output_cluster_pca<-kmeans(pca_vinhos,80, iter = 50)
cluster_vinho_pca<-output_cluster_pca$cluster

table (cluster_vinho_pca)

clusplot(pca_vinhos, output_cluster_pca$cluster, color=TRUE, shade=TRUE, labels=2, lines=0 , cex=0.75)
```
Concluímos que utilizando os componentes principais teremos um número menor de clusters porém com características que possibilitam uma melhor visualização desses agrupamentos.